{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 06_Object_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ePvMYCh4oB2n"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9d91790e638c40e2b58c51a77a9217dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_805bfae2eec54941aa79b641530467f9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1caed85c25c44ab19f251fc39f9f61fb",
              "IPY_MODEL_df45ec4832ab415b9700bb0d884a6e3c"
            ]
          }
        },
        "805bfae2eec54941aa79b641530467f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1caed85c25c44ab19f251fc39f9f61fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1c4611a453a54e0e995b75cb7335b5a5",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 87306240,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 87306240,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2595d5fb42824dc596e5bed9b64e1ddb"
          }
        },
        "df45ec4832ab415b9700bb0d884a6e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7a02da85cd5c4c14879beddf92502829",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 83.3M/83.3M [00:09&lt;00:00, 9.58MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_19ad0ae71cc1453da088660809077470"
          }
        },
        "1c4611a453a54e0e995b75cb7335b5a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2595d5fb42824dc596e5bed9b64e1ddb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a02da85cd5c4c14879beddf92502829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "19ad0ae71cc1453da088660809077470": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idrisr/colab-notebooks/blob/main/Copy_of_06_Object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b55MJA5oB1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f254c005-d213-4ac5-cea5-bcda00324381"
      },
      "source": [
        "#Run once per session\n",
        "!pip install fastai -q --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 194kB 20.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 776.8MB 22kB/s \n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/torchvision/\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 241kB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.2MB/s \n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lx8jjQMrx8s"
      },
      "source": [
        "# Object Detection\n",
        "\n",
        "Finding the localized area in which an object presides from two points, the bottom left and top right\n",
        "\n",
        "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/06/maxresdefault.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "422XnPu9oB1y"
      },
      "source": [
        "from fastai.vision.all import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxEIggy-oB10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "aaba606e-c9cf-44f3-bfd6-adbb74e84b2c"
      },
      "source": [
        "path = untar_data(URLs.PASCAL_2007)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM9FeIxboB13"
      },
      "source": [
        "Now how do we get our labels? `fastai` has a `get_annotations` function that we can use to grab the image and their bounding box. The one-line documentation states:\n",
        "\"Open a COCO style json in `fname` and returns the list of filenames (with mabye `prefix`) and labelled bounding boxes.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHmSV1cMoB13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1830e30b-a225-47dc-8e10-fa1160d501dd"
      },
      "source": [
        "path.ls()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#8) [Path('/root/.fastai/data/pascal_2007/segmentation'),Path('/root/.fastai/data/pascal_2007/test'),Path('/root/.fastai/data/pascal_2007/train.csv'),Path('/root/.fastai/data/pascal_2007/test.csv'),Path('/root/.fastai/data/pascal_2007/train.json'),Path('/root/.fastai/data/pascal_2007/train'),Path('/root/.fastai/data/pascal_2007/valid.json'),Path('/root/.fastai/data/pascal_2007/test.json')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E5vCutmoB15"
      },
      "source": [
        "We'll want to read out of the `train.json`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLfU8hv8oB15"
      },
      "source": [
        "imgs, lbl_bbox = get_annotations(path/'train.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zmQRxdzoB17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e50f826d-5331-4455-fb85-97eb1125d2f3"
      },
      "source": [
        "imgs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'000012.jpg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0AprSfooB1-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3fd5c0c5-375e-454b-de3c-7f8ea7897f1c"
      },
      "source": [
        "lbl_bbox[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[155, 96, 351, 270]], ['car'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJHvjVsWoB2A"
      },
      "source": [
        "Next, we want to be able to quickly look up a corresponding image to it's label. We'll use a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSWMCLRZoB2A"
      },
      "source": [
        "img2bbox = dict(zip(imgs, lbl_bbox))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP2IcI1BoB2C"
      },
      "source": [
        "Let's check the first item"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J96xwN-6oB2D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "baee7bb3-1772-4f2f-d1d5-de15e11465fe"
      },
      "source": [
        "first = {k: img2bbox[k] for k in list(img2bbox)[:1]}; first"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'000012.jpg': ([[155, 96, 351, 270]], ['car'])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CENSeAMVoB2E"
      },
      "source": [
        "Great! Now let's build our `DataBlock`. We'll have two outputs, the bounding box itself and a label, with one input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw3xrPPyoB2F"
      },
      "source": [
        "getters = [lambda o: path/'train'/o, lambda o: img2bbox[o][0], lambda o: img2bbox[o][1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VBzZ1jAoB2H"
      },
      "source": [
        "For our transforms, we'll use some of the ones we defined earlier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0ZPlZuRoB2H"
      },
      "source": [
        "item_tfms = [Resize(128, method='pad'),]\n",
        "batch_tfms = [Rotate(), Flip(), Dihedral(), Normalize.from_stats(*imagenet_stats)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E61v1CQoqjNS"
      },
      "source": [
        "Why do we need a custom `get_images`? Because we want our **images** that came back to us, not the entire folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDX1YnuLqmTY"
      },
      "source": [
        "def get_train_imgs(noop):  return imgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHefiSB3oB2L"
      },
      "source": [
        "We'll now make our `DataBlock`. We want to adjust `n_inp` as we expect two outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bY3RjdNoB2L"
      },
      "source": [
        "pascal = DataBlock(blocks=(ImageBlock, BBoxBlock, BBoxLblBlock),\n",
        "                 splitter=RandomSplitter(),\n",
        "                 get_items=get_train_imgs,\n",
        "                 getters=getters,\n",
        "                 item_tfms=item_tfms,\n",
        "                 batch_tfms=batch_tfms,\n",
        "                 n_inp=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvI6b6aioB2N"
      },
      "source": [
        "dls = pascal.dataloaders(path/'train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Es7TtheoB2P"
      },
      "source": [
        "dls.c = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPa2W_x1oB2R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "be974204-0f83-4b88-a00f-edf05a1846cc"
      },
      "source": [
        "dls.show_batch()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-90634fcc3c9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dls' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBn8v8sgoB2T"
      },
      "source": [
        "# The Model\n",
        "\n",
        "The architecture we are going to use is called `RetinaNet`. I've exported this all myself for you guys to use quickly, if you want to explore what's going on in the code I'd recommend the Object Detection lesson [here](https://www.youtube.com/watch?v=Z0ssNAbe81M&t=4496s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcZDuQ1uoB2T"
      },
      "source": [
        "Let's clone my repo and work out of it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMxjDbNvoB2U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "79c9be5e-1959-495a-b769-f5e5ed072fbb"
      },
      "source": [
        "!git clone https://github.com/muellerzr/Practical-Deep-Learning-for-Coders-2.0.git\n",
        "%cd \"Practical-Deep-Learning-for-Coders-2.0/Computer Vision\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Practical-Deep-Learning-for-Coders-2.0'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 1253 (delta 9), reused 10 (delta 2), pack-reused 1227\u001b[K\n",
            "Receiving objects: 100% (1253/1253), 72.22 MiB | 36.14 MiB/s, done.\n",
            "Resolving deltas: 100% (796/796), done.\n",
            "/content/Practical-Deep-Learning-for-Coders-2.0/Computer Vision\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUsSJvE8oB2W"
      },
      "source": [
        "from imports import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9y7IZP2oB2X"
      },
      "source": [
        "We're still going to use transfer learning here by creating an `encoder` (body) of our model and a head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0CrU5CkoB2Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "9d91790e638c40e2b58c51a77a9217dc",
            "805bfae2eec54941aa79b641530467f9",
            "1caed85c25c44ab19f251fc39f9f61fb",
            "df45ec4832ab415b9700bb0d884a6e3c",
            "1c4611a453a54e0e995b75cb7335b5a5",
            "2595d5fb42824dc596e5bed9b64e1ddb",
            "7a02da85cd5c4c14879beddf92502829",
            "19ad0ae71cc1453da088660809077470"
          ]
        },
        "outputId": "03f0935e-a5ce-4ac7-972f-12f86cbbd1ce"
      },
      "source": [
        "encoder = create_body(resnet34, pretrained=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d91790e638c40e2b58c51a77a9217dc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=87306240.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plv1hyUGoB2Z"
      },
      "source": [
        "Now that we have our encoder, we can call the `RetinaNet` architecture. We'll pass in the encoder, the number of classes, and what we want our final bias to be on the last convolutional layer (how we initialize our model). Jeremy has his example at -4 so let's use this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtPgStQtoB2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96520d75-e28d-448d-9036-dd66eea7a55f"
      },
      "source": [
        "get_c(dls)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g00phNe2oB2c"
      },
      "source": [
        "arch = RetinaNet(encoder, get_c(dls), final_bias=-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQzm_ni7oB2d"
      },
      "source": [
        "Another big difference is the head of our model. Instead of our linear layers with pooling layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECAKT4quoB2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "d59662c8-9dd5-460b-8c6f-037842f9ef7c"
      },
      "source": [
        "create_head(124, 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): AdaptiveConcatPool2d(\n",
              "    (ap): AdaptiveAvgPool2d(output_size=1)\n",
              "    (mp): AdaptiveMaxPool2d(output_size=1)\n",
              "  )\n",
              "  (1): Flatten(full=False)\n",
              "  (2): BatchNorm1d(124, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (3): Dropout(p=0.25, inplace=False)\n",
              "  (4): Linear(in_features=124, out_features=512, bias=False)\n",
              "  (5): ReLU(inplace=True)\n",
              "  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (7): Dropout(p=0.5, inplace=False)\n",
              "  (8): Linear(in_features=512, out_features=4, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHpWUYCsoB2f"
      },
      "source": [
        "We have one with a smoother, a classifer, and a `box_regressor` (to get our points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fORf5ZcPoB2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "19e76f92-7860-406c-c089-0369a9eaf8b6"
      },
      "source": [
        "arch.smoothers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModuleList(\n",
              "  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw9ypuWYoB2h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "79b3645b-5ec6-4d03-defb-ad4cdac4d08f"
      },
      "source": [
        "arch.classifier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): ConvLayer(\n",
              "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (1): ConvLayer(\n",
              "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (2): ConvLayer(\n",
              "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (3): ConvLayer(\n",
              "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (4): Conv2d(256, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOllk6w9oB2j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "e6eccb71-36c5-43ad-e7e6-22078f0c339e"
      },
      "source": [
        "arch.box_regressor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): ConvLayer(\n",
              "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (1): ConvLayer(\n",
              "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (2): ConvLayer(\n",
              "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (3): ConvLayer(\n",
              "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (4): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePvMYCh4oB2n"
      },
      "source": [
        "## Loss Function\n",
        "Now we can move onto our loss function. For RetinaNet to work, we need to define what the aspect ratio's and scales of our image should be. The paper used [1,2**(1/3), 2**(2/3)], but they also used an image size of 600 pixels, so even the largest feature map (box) gave anchors that covered less than the image. But for us it would go over. As such we will use -1/3 and -2/3 instead. We will need these for inference later!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICN19VOnoB2n"
      },
      "source": [
        "ratios = [1/2,1,2]\n",
        "scales = [1,2**(-1/3), 2**(-2/3)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOukn6h4oB2p"
      },
      "source": [
        "Let's make our loss function, which is `RetinaNetFocalLoss`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1EgEMYQoB2p"
      },
      "source": [
        "crit = RetinaNetFocalLoss(scales=scales, ratios=ratios)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0A6DCu3oB2r"
      },
      "source": [
        "Now let's make our `Learner`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBTmB2eUoB2r"
      },
      "source": [
        "We want to freeze our `encoder` and keep everything else unfrozen to start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMYuwnR0oB2s"
      },
      "source": [
        "def _retinanet_split(m): return L(m.encoder,nn.Sequential(m.c5top6, m.p6top7, m.merges, m.smoothers, m.classifier, m.box_regressor)).map(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8kochl-oB2t"
      },
      "source": [
        "learn = Learner(dls, arch, loss_func=crit, splitter=_retinanet_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDUhW4yAoB2v"
      },
      "source": [
        "learn.freeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDZWhZlgoB2y"
      },
      "source": [
        "Now let's train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHBmZg-CoB21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "18e95aa9-0715-4b2e-91fd-7fad8f8d2e0e"
      },
      "source": [
        "learn.fit_one_cycle(10, slice(1e-5, 1e-4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.256616</td>\n",
              "      <td>2.996806</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.862086</td>\n",
              "      <td>2.687984</td>\n",
              "      <td>00:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.666158</td>\n",
              "      <td>2.688018</td>\n",
              "      <td>00:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.561396</td>\n",
              "      <td>2.667681</td>\n",
              "      <td>00:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.478174</td>\n",
              "      <td>2.413936</td>\n",
              "      <td>00:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.423190</td>\n",
              "      <td>2.493899</td>\n",
              "      <td>00:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.339488</td>\n",
              "      <td>2.480800</td>\n",
              "      <td>00:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.278033</td>\n",
              "      <td>2.479896</td>\n",
              "      <td>00:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.239934</td>\n",
              "      <td>2.422191</td>\n",
              "      <td>00:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.204041</td>\n",
              "      <td>2.403617</td>\n",
              "      <td>00:29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouxRYNyuoB23"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}